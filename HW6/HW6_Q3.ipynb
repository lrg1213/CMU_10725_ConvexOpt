{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import time\n",
    "from math import exp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "# X_train, y_train = make_classification(n_samples=10000)\n",
    "# X_test, y_test = make_classification(n_samples=1000)\n",
    "X, y = make_classification(n_samples=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda z : 1 / (1 + np.exp(-z))\n",
    "logloss = lambda y_hat, y : np.sum(-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)) / len(y_hat)\n",
    "predict = lambda X: sigmoid(np.dot(X, betas.T)) > .5\n",
    "predict_with_output = lambda X: (X > .5) * 1\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, beta, lr):\n",
    "    y = y.reshape(-1, 1)\n",
    "    gradients = np.dot(X.T, sigmoid(np.dot(X, beta.T)) - y) / len(y)\n",
    "    new_betas = beta - lr * gradients.T\n",
    "\n",
    "    return new_betas\n",
    "\n",
    "def prepare_batches(X, y, batch_size):\n",
    "    X_batch_list = list()\n",
    "    y_batch_list = list()\n",
    "    \n",
    "    for i in range(len(y) // batch_size):\n",
    "        X_batch_list.append(X[i * batch_size : i * batch_size + batch_size, :])\n",
    "        y_batch_list.append(y[i * batch_size : i * batch_size + batch_size])\n",
    "    \n",
    "    if len(y) % batch_size > 0:\n",
    "        X_batch_list.append(X[len(y) // batch_size * batch_size:, :])\n",
    "        y_batch_list.append(y[len(y) // batch_size * batch_size:])\n",
    "\n",
    "    return X_batch_list, y_batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = np.random.permutation(len(X))\n",
    "\n",
    "X, y = np.asarray(X).squeeze(), np.asarray(y)\n",
    "\n",
    "X = X[permutations, :]\n",
    "y = y[permutations]\n",
    "\n",
    "#To add beta 0\n",
    "temp = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "temp[:, 1:] = X\n",
    "X = temp\n",
    "\n",
    "len_test = len(X) // 5 \n",
    "len_train = len(X) - len_test\n",
    "X_test, y_test, X_train, y_train = X[:len_test, :], y[:len_test], X[len_test:, :], y[len_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X matrix is: (10000, 21)\n",
      "Shape of y matrix is: (10000,)\n",
      "Shape of X_test matrix is: (10000, 21)\n",
      "Shape of y_test matrix is: (10000, 21)\n",
      "Shape of X_train matrix is: (10000, 21)\n",
      "Shape of y_train matrix is: (10000, 21)\n",
      "Desired samples feature vector: [ 1.         -0.51926415  0.5940006  -0.93599479  0.53273792 -1.23959984\n",
      " -0.64952957  0.26893774  1.0919569  -0.34649526 -1.20836382  0.69807529\n",
      "  0.41281416  0.62505391 -1.91812173  0.61770769  0.6808084   1.24744623\n",
      " -1.00572358  0.94005728 -1.18242773]\n",
      "Desired samples ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X matrix is: \" + str(X.shape))\n",
    "print(\"Shape of y matrix is: \" + str(y.shape))\n",
    "print(\"Shape of X_test matrix is: \" + str(X.shape))\n",
    "print(\"Shape of y_test matrix is: \" + str(X.shape))\n",
    "print(\"Shape of X_train matrix is: \" + str(X.shape))\n",
    "print(\"Shape of y_train matrix is: \" + str(X.shape))\n",
    "\n",
    "print(\"Desired samples feature vector: \" + str(X[2]))\n",
    "print(\"Desired samples ground truth: \" + str(y[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = np.random.random(X.shape[1]).reshape(1, -1)\n",
    "\n",
    "lr = 0.1\n",
    "batch_size = 128\n",
    "n_iterations = 10000\n",
    "\n",
    "patience = 2\n",
    "min_delta = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_hist = list()\n",
    "test_error_hist = list()\n",
    "test_acc_hist = list()\n",
    "\n",
    "X_batch_list, y_batch_list = prepare_batches(X_train, y_train, batch_size)\n",
    "\n",
    "n_batches = len(y_batch_list)\n",
    "\n",
    "prev_average = 10000\n",
    "\n",
    "patience_counter = 0\n",
    "iteration_counter = 0 \n",
    "while iteration_counter < n_iterations:\n",
    "    for i in range(n_batches):\n",
    "        X_batch = X_batch_list[i]\n",
    "        y_batch = y_batch_list[i]\n",
    "\n",
    "        betas = gradient_descent(X_batch, y_batch, betas, lr)\n",
    "        \n",
    "        y_hat = sigmoid(np.dot(X_batch, betas.T))\n",
    "        train_error_hist.append(logloss(y_hat, y_batch) / len(y_batch))\n",
    "              \n",
    "        y_hat = sigmoid(np.dot(X_test, betas.T))\n",
    "        test_error_hist.append(logloss(y_hat, y_test) / len(y_test))\n",
    "        test_acc_hist.append(np.mean((predict_with_output(y_hat) == y_test.reshape(-1, 1)) * 1))\n",
    "          \n",
    "        iteration_counter += 1\n",
    "        \n",
    "    current_average = np.mean(train_error_hist[-n_batches:])\n",
    "        \n",
    "    if np.abs(prev_average - current_average) < min_delta:\n",
    "        patience_counter += 1\n",
    "    else:\n",
    "        patience_counter = 0\n",
    "        \n",
    "    prev_average = current_average\n",
    "    \n",
    "    if patience_counter == patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_error_hist)\n",
    "plt.plot(train_error_hist)\n",
    "plt.xlabel(\"#Iterations\")\n",
    "plt.ylabel(\"Total Loss\")\n",
    "plt.title(\"Loss vs Number of iterations\")\n",
    "plt.legend((\"Test error\", \"Train error\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_acc_hist)\n",
    "plt.xlabel(\"#Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Number of iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]*np.ones(row.shape[0])\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return 1.0 / (1.0 + exp(-yhat))\n",
    "\n",
    "def bgd(X_train, y_train, l_rate, n_epoch, coef = None):\n",
    "    if coef==None:\n",
    "        coef = [0.0 for i in range(X_train.shape[1])]\n",
    "    for epoch in range(n_epoch):\n",
    "        for i in range(X_train.shape[0]):\n",
    "            row = X[i]\n",
    "\n",
    "            yhat = predict(row, coef)\n",
    "            error = y_train[i] - yhat\n",
    "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "    return coef  \n",
    "\n",
    "def mini_bgd(X_train, y_train, l_rate, n_epoch, coef = None):\n",
    "    if coef==None:\n",
    "        coef = [0.0 for i in range(X_train.shape[1])]\n",
    "    for epoch in range(n_epoch):\n",
    "        np.random.shuffle(X_train)\n",
    "        for i in range(int(X_train.shape[0]/50)):\n",
    "            row = X[i]\n",
    "            yhat = predict(row, coef)\n",
    "            error = y_train[i] - yhat\n",
    "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "    return coef    \n",
    "\n",
    "def sgd(X_train, y_train, l_rate, n_epoch, batch_size=100, coef = None):\n",
    "    idx = np.arange(0,X_train.shape[0])\n",
    "    if coef==None:\n",
    "        coef = [0.0 for i in range(X_train.shape[1])]\n",
    "    for epoch in range(n_epoch):\n",
    "        np.random.shuffle(idx)\n",
    "        for i in range(int(idx.shape[0]/batch_size)):\n",
    "            row = X_train[idx[i*batch_size:(i+1)*batch_size]]\n",
    "            yhat = predict(row,coef)\n",
    "            error = y_train[idx[i*batch_size:(i+1)*batch_size]] - yhat\n",
    "            coef[idx[i*batch_size:(i+1)*batch_size]] = coef[idx[i*batch_size:(i+1)*batch_size]] + l_rate * error * yhat * (1.0 - yhat) *row\n",
    "#         for i in range(int(X_train.shape[0]/50)):\n",
    "#             row = X[i]\n",
    "#             yhat = predict(row, coef)\n",
    "#             error = y_train[i] - yhat\n",
    "#             coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "#             for i in range(len(row)-1):\n",
    "#                 coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "    return coef\n",
    "    \n",
    "\n",
    "def logistic_regression(X_train, y_train, X_test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    coef = sgd(X_train, y_train, l_rate, n_epoch)\n",
    "    for row in X_test:\n",
    "        yhat = predict(row, coef)\n",
    "        yhat = round(yhat)\n",
    "        predictions.append(yhat)\n",
    "    return(predictions)\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "\n",
    "def evaluate_algorithm(X, y, algorithm, n_folds, *args):\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "    kf.get_n_splits(X)\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        predicted = algorithm(X[train_index], y[train_index], X[test_index], *args)\n",
    "        actual = y[test_index]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate algorithm\n",
    "n_folds = 4\n",
    "l_rate = 0.1\n",
    "n_epoch = 100\n",
    "start = time.process_time() \n",
    "scores = evaluate_algorithm(X, y, logistic_regression, n_folds, l_rate, n_epoch)\n",
    "elapsed = (time.process_time() - start)\n",
    "print(\"Time used:\",elapsed)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
