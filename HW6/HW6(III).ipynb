{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c49b9d78810c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time used:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c49b9d78810c>\u001b[0m in \u001b[0;36mevaluate_algorithm\u001b[0;34m(dataset, algorithm, n_folds, *args)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mrow_copy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c49b9d78810c>\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(train, test, l_rate, n_epoch)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoefficients_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;31m# coef = distributed_coefficients_sgd(train, l_rate, n_epoch, num_workers=8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c49b9d78810c>\u001b[0m in \u001b[0;36mcoefficients_sgd\u001b[0;34m(train, l_rate, n_epoch, coef)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mcoef\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c49b9d78810c>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(row, coefficients)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0myhat\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcoefficients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Logistic Regression on Diabetes Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import shuffle\n",
    "from csv import reader\n",
    "from math import exp\n",
    "import numpy as np\n",
    "import time\n",
    "# from multiprocessing import Pool\n",
    "from multiprocessing import Pool as ThreadPool\n",
    "\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return 1.0 / (1.0 + exp(-yhat))\n",
    "\n",
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch, coef = None):\n",
    "    if coef==None:\n",
    "        coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = row[-1] - yhat\n",
    "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "    return coef\n",
    "    \n",
    "\n",
    "# Linear Regression Algorithm With Stochastic Gradient Descent\n",
    "def logistic_regression(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    coef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "    # coef = distributed_coefficients_sgd(train, l_rate, n_epoch, num_workers=8)\n",
    "    for row in test:\n",
    "        yhat = predict(row, coef)\n",
    "        yhat = round(yhat)\n",
    "        predictions.append(yhat)\n",
    "    return(predictions)\n",
    "\n",
    "# Test the logistic regression algorithm on the diabetes dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'pima-indians-diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# normalize\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 10\n",
    "l_rate = 0.1\n",
    "n_epoch = 1000\n",
    "start = time.process_time() \n",
    "scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)\n",
    "elapsed = (time.process_time() - start)\n",
    "print(\"Time used:\",elapsed)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "\n",
    "class FixedStepSizeSelector:\n",
    "    def __init__(self, step_size):\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def calc_step_size(self, loss_func, grad_func, cur_loss, cur_grad, params_to_solve):\n",
    "        return self.step_size\n",
    "\n",
    "\n",
    "class BacktrackingStepSizeSelector:\n",
    "    def __init__(self, step_size_min_reduction_relative_to_grad=0.3, step_size_reduction_factor=0.5,\n",
    "                 initial_step_size=2.0):\n",
    "        self.step_size = initial_step_size\n",
    "        self.step_size_min_reduction_relative_to_grad = step_size_min_reduction_relative_to_grad\n",
    "        self.step_size_reduction_factor = step_size_reduction_factor\n",
    "        self.min_grad_mag_for_backtracking = 1e-10\n",
    "        self.min_step_size = 1e-10\n",
    "        self.max_step_size = 100\n",
    "\n",
    "    def calc_step_size(self, loss_func, grad_func, cur_loss, cur_grad, params_to_solve):\n",
    "        step_size_criteria = False\n",
    "        self.step_size /= self.step_size_reduction_factor ** 2\n",
    "        grad_mag = np.sum(cur_grad * cur_grad)\n",
    "        while not step_size_criteria:\n",
    "            self.step_size *= self.step_size_reduction_factor\n",
    "            lhs = loss_func(params_to_solve - self.step_size * grad_func(params_to_solve))\n",
    "\n",
    "            if grad_mag < self.min_grad_mag_for_backtracking \\\n",
    "                    or self.step_size < self.min_step_size \\\n",
    "                    or self.step_size > self.max_step_size:\n",
    "                self.step_size = np.maximum(self.min_step_size, np.minimum(self.max_step_size, self.step_size))\n",
    "                step_size_criteria = True\n",
    "            else:\n",
    "                rhs = cur_loss - self.step_size_min_reduction_relative_to_grad * self.step_size * grad_mag\n",
    "                step_size_criteria = lhs <= rhs\n",
    "\n",
    "        return self.step_size\n",
    "\n",
    "\n",
    "class GradientDescent:\n",
    "    def __init__(self, tol=1e-10, abstol=1e-14, max_iter=10000, step_size_selector=BacktrackingStepSizeSelector(),\n",
    "                 is_verbose=False, store_iter_loss=False):\n",
    "        self.step_size_selector = step_size_selector\n",
    "        self.store_iter_loss = store_iter_loss\n",
    "        self.is_verbose = is_verbose\n",
    "\n",
    "        # Termination criteria\n",
    "        self.reltol = tol\n",
    "        self.abstol = abstol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        if self.store_iter_loss:\n",
    "            self.iter_loss = []\n",
    "        else:\n",
    "            self.iter_loss = None\n",
    "\n",
    "    def fit(self, loss_func, loss_grad_func, initial_guess, num_samples):\n",
    "    # TODO: Unify the declaration of this function in this class and StochasticGradientDescent.\n",
    "        params_to_solve = initial_guess.copy()\n",
    "        termination = False\n",
    "        it = 0\n",
    "\n",
    "        prev_loss = np.inf\n",
    "        while not termination:\n",
    "            cur_loss = loss_func(params_to_solve)\n",
    "            cur_grad = loss_grad_func(params_to_solve)\n",
    "\n",
    "            step_size = self.step_size_selector.calc_step_size(\n",
    "                loss_func, loss_grad_func, cur_loss, cur_grad, params_to_solve)\n",
    "\n",
    "            cur_loss = loss_func(params_to_solve)\n",
    "            params_update = step_size * loss_grad_func(params_to_solve)\n",
    "\n",
    "            termination = self.termination_criteria(cur_loss / num_samples, prev_loss / num_samples, params_update, it,\n",
    "                                                    self.max_iter, self.reltol,\n",
    "                                                    self.abstol)\n",
    "            params_to_solve -= params_update\n",
    "\n",
    "            prev_loss = cur_loss\n",
    "            it += 1\n",
    "\n",
    "            if self.store_iter_loss:\n",
    "                self.iter_loss.append(cur_loss)\n",
    "\n",
    "            if self.is_verbose and np.mod(it, 1) == 0:\n",
    "                print ('Iteration {}: loss={}. step_size={}.\\n  b={}\\n, update={}'.format(\n",
    "                    it, cur_loss / 100000, step_size, params_to_solve, params_update))\n",
    "        return params_to_solve\n",
    "\n",
    "    def termination_criteria(self, cur_loss, prev_loss, update, it, max_iter, reltol, abstol):\n",
    "        termination = False\n",
    "        abstol_criterion = np.max(np.abs(update)) < abstol\n",
    "        if cur_loss != 0:\n",
    "            reltol_criterion = np.abs(prev_loss - cur_loss) / cur_loss < reltol\n",
    "        else:\n",
    "            reltol_criterion = False\n",
    "        if abstol_criterion or reltol_criterion or it >= max_iter:\n",
    "            termination = True\n",
    "        return termination\n",
    "\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\" Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i+n]\n",
    "\n",
    "class StochasticGradientDescent:\n",
    "    PARAMS_MIX_FACTOR = 0.9\n",
    "\n",
    "    def __init__(self, tol=1e-10, abstol=1e-14, max_iter=10, step_size_selector=BacktrackingStepSizeSelector(),\n",
    "                 is_verbose=False, store_iter_loss=False, batch_size=128, num_passes=1000):\n",
    "        self.num_passes = num_passes\n",
    "        self.batch_size = batch_size\n",
    "        self.tol = tol\n",
    "        self.abstol = abstol\n",
    "        self.max_iter = max_iter\n",
    "        self.store_iter_loss = store_iter_loss\n",
    "        self.is_verbose = is_verbose\n",
    "        self.step_size_selector = step_size_selector\n",
    "        \n",
    "        if self.store_iter_loss:\n",
    "            self.iter_loss = []\n",
    "        else:\n",
    "            self.iter_loss = None\n",
    "\n",
    "    def fit(self, loss_func, loss_grad_func, initial_guess, X, y):\n",
    "        for i in range(0, self.num_passes):\n",
    "            obs_indices = list(range(0, len(y)))\n",
    "            random.shuffle(obs_indices)\n",
    "            for chunk_num, chunk_ind in enumerate(chunks(obs_indices, self.batch_size)):\n",
    "                if len(chunk_ind) < self.batch_size:\n",
    "                    break\n",
    "                gd = GradientDescent(tol=self.tol, abstol=self.abstol,\n",
    "                                     max_iter=self.max_iter, is_verbose=False,\n",
    "                                     store_iter_loss=False,\n",
    "                                     step_size_selector=self.step_size_selector)\n",
    "                X_chunk = X[chunk_ind, :]\n",
    "                y_chunk = y[chunk_ind]\n",
    "                \n",
    "\n",
    "                fit_result = gd.fit(lambda b : loss_func(X_chunk, y_chunk, b),\n",
    "                                       lambda b : loss_grad_func(X_chunk, y_chunk, b),\n",
    "                                       initial_guess, num_samples=X_chunk.shape[0])\n",
    "                initial_guess = self.PARAMS_MIX_FACTOR * initial_guess + (1 - self.PARAMS_MIX_FACTOR) * fit_result\n",
    "            \n",
    "                if self.is_verbose:\n",
    "                    print ('{} | {} | {}'.format(chunk_num, initial_guess, len(chunk_ind)))\n",
    "            if self.store_iter_loss:\n",
    "                self.iter_loss.append(loss_func(X,y,initial_guess))\n",
    "        return initial_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "BOUND = 20.0\n",
    "\n",
    "\n",
    "def sigmoid(b, x):\n",
    "    return 1.0 / (1.0 + np.exp(np.minimum(BOUND, np.maximum(-BOUND, -np.dot(b, x)))))\n",
    "\n",
    "\n",
    "def lr_loss_gradient(x, y, b, reg_param=0.0):\n",
    "    grad = (np.dot(np.transpose((sigmoid(x, np.transpose(b)))), x) - np.dot(np.transpose(x), y))[0, :]\n",
    "    grad += 2.0 * reg_param * b[0, :]\n",
    "    return grad\n",
    "\n",
    "\n",
    "def lr_loss(X, y, b, reg_param=0.0):\n",
    "    loss = -np.sum(np.multiply(np.log(sigmoid(b, np.transpose(X))), np.transpose(y))\n",
    "                   + np.multiply(np.log(1.0 - sigmoid(b, np.transpose(X))), 1.0 - np.transpose(y)))\n",
    "\n",
    "    reg = reg_param * np.dot(b, np.transpose(b))\n",
    "    loss += reg[0]\n",
    "    return loss[0]\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, reg_param=0.0, is_verbose=False, store_iter_loss=False, solver='sgd', step_size='auto'):\n",
    "        self.solver = solver\n",
    "        self.step_size = step_size\n",
    "        self.store_iter_loss = store_iter_loss\n",
    "        self.reg_param = reg_param\n",
    "        self.is_verbose = is_verbose\n",
    "        self.iter_loss = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        opt_loss_func = lambda b: lr_loss(X, y, b, self.reg_param)\n",
    "        opt_grad_loss_func = lambda b: lr_loss_gradient(X, y, b, self.reg_param)\n",
    "\n",
    "        if self.step_size == 'auto':\n",
    "            step_size_selector = BacktrackingStepSizeSelector()\n",
    "        else:\n",
    "            step_size_selector = FixedStepSizeSelector(step_size=self.step_size)\n",
    "\n",
    "        if self.solver == 'gd':\n",
    "            gd = GradientDescent(is_verbose=self.is_verbose, store_iter_loss=self.store_iter_loss,\n",
    "                                              step_size_selector=step_size_selector)\n",
    "            if self.store_iter_loss:\n",
    "                self.iter_loss = gd.iter_loss\n",
    "\n",
    "            self.coef = gd.fit(opt_loss_func, opt_grad_loss_func, np.zeros((1, X.shape[1])), num_samples=X.shape[0])\n",
    "        elif self.solver == 'sgd':\n",
    "            sgd = StochasticGradientDescent(is_verbose=self.is_verbose, store_iter_loss=self.store_iter_loss,\n",
    "                                              step_size_selector=step_size_selector)\n",
    "            \n",
    "            self.coef = sgd.fit(lambda X,y,b: lr_loss(X, y, b, self.reg_param),\n",
    "                                lambda X,y,b: lr_loss_gradient(X, y, b, self.reg_param),\n",
    "                                np.zeros((1, X.shape[1])),X, y)\n",
    "            if self.store_iter_loss:\n",
    "                self.iter_loss = sgd.iter_loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        return sigmoid(self.coef, np.transpose(X))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "(768,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# N = 1000\n",
    "# K = 10\n",
    "# X = np.random.randn(N,K)\n",
    "# y = np.random.randint(0,2,N)\n",
    "\n",
    "X = np.array(dataset)[:,:-1]\n",
    "y = np.array(dataset)[:,-1]\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "lr_fixed_step = LogisticRegression(reg_param=0.01, store_iter_loss=True, step_size=0.01)\n",
    "lr_fixed_step.fit(X, y)\n",
    "\n",
    "lr_auto_step = LogisticRegression(reg_param=0.01, store_iter_loss=True, step_size='auto')\n",
    "lr_auto_step.fit(X, y)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(lr_fixed_step.iter_loss)), lr_fixed_step.iter_loss, 'r') \n",
    "plt.plot(np.arange(0,len(lr_auto_step.iter_loss)), lr_auto_step.iter_loss, 'g')\n",
    "plt.legend(['Step size - 0.01', 'Step size - auto'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
