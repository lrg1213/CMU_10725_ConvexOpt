{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 14.801093909999992\n",
      "Scores: [73.8562091503268, 78.43137254901961, 81.69934640522875, 75.81699346405229, 75.16339869281046]\n",
      "Mean Accuracy: 76.993%\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression on Diabetes Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import shuffle\n",
    "from csv import reader\n",
    "from math import exp\n",
    "import numpy as np\n",
    "import time\n",
    "# from multiprocessing import Pool\n",
    "from multiprocessing import Pool as ThreadPool\n",
    "\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return 1.0 / (1.0 + exp(-yhat))\n",
    "\n",
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch, coef = None):\n",
    "    if coef==None:\n",
    "        coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = row[-1] - yhat\n",
    "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "    return coef\n",
    "\n",
    "def coef_sgd(para_list):\n",
    "    train = para_list[0]\n",
    "    l_rate = para_list[1]\n",
    "    coef = para_list[2]\n",
    "    \n",
    "    for row in train:\n",
    "        yhat = predict(row, coef)\n",
    "        error = row[-1] - yhat\n",
    "        coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "        for i in range(len(row)-1):\n",
    "            coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "    return coef\n",
    "\n",
    "def distributed_coefficients_sgd(train,l_rate,n_epoch,num_workers):\n",
    "    coef_avg = np.array([0.0 for i in range(len(train[0]))])\n",
    "    pool = ThreadPool()\n",
    "    for epoch in range(n_epoch):\n",
    "        shuffle(train)\n",
    "        n = len(train)\n",
    "        n_per_worker = int(n/num_workers)\n",
    "        worker_list = []\n",
    "        for i in range(num_workers):\n",
    "            worker_list.append([train[i*n_per_worker:(i+1)*n_per_worker],l_rate,coef_avg.tolist()])\n",
    "        results = pool.map(coef_sgd, worker_list)\n",
    "#         results = []\n",
    "#         for i in range(num_workers):\n",
    "#             train_i = train[i*n_per_worker:(i+1)*n_per_worker]\n",
    "#             results.append(coefficients_sgd(train_i, l_rate, 1, coef_avg.tolist()))\n",
    "        coef_avg = np.array(results).mean(axis=0)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return coef_avg\n",
    "    \n",
    "\n",
    "# Linear Regression Algorithm With Stochastic Gradient Descent\n",
    "def logistic_regression(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    # coef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "    coef = distributed_coefficients_sgd(train, l_rate, n_epoch, num_workers=8)\n",
    "    for row in test:\n",
    "        yhat = predict(row, coef)\n",
    "        yhat = round(yhat)\n",
    "        predictions.append(yhat)\n",
    "    return(predictions)\n",
    "\n",
    "# Test the logistic regression algorithm on the diabetes dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'pima-indians-diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# normalize\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.1\n",
    "n_epoch = 1000\n",
    "start = time.process_time() \n",
    "scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)\n",
    "elapsed = (time.process_time() - start)\n",
    "print(\"Time used:\",elapsed)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 17:38:10,689 - apple [INFO] item: apple\n",
      "2019-12-05 17:38:10,690 - bananan [INFO] item: bananan\n",
      "2019-12-05 17:38:10,691 - cake [INFO] item: cake\n",
      "2019-12-05 17:38:10,691 - dumpling [INFO] item: dumpling\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "def get_logger(name):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s [%(levelname)s] %(message)s')\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)\n",
    "    return logger\n",
    "def process(item):\n",
    "    log = get_logger(item)\n",
    "    log.info(\"item: %s\" % item)\n",
    "    time.sleep(5)\n",
    "    return item[0]\n",
    "\n",
    "items = ['apple', 'bananan', 'cake', 'dumpling']\n",
    "pool = ThreadPool()\n",
    "results = pool.map(process, items)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
